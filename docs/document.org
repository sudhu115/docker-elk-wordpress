#+Title: Docker ELK Wordpress Project
#+Author: Yogesh Agrawal
#+Email: yogeshiiith@gmail.com
#+Date: <2016-06-16 Thu>

* Introduction
  This document describes the design and implementation of the
  docker compose based application deployment and centralized logging
  using Elasticsearch, Logstash and Kibana stack.
* Design
  The infrastructure is setup in the cloud environment - AWS. An EC2
  instance is created with the minimal requirement. The rational
  behind resource requirement of an EC2 instance is discussed below.
  
  Inside the EC2 instance, the services are setup in a container based
  system. Here we have mainly five services:
  1. Elasticsearch
  2. Logstash
  3. Kibana
  4. Application - Wordpress
  5. Mysql database

  So we will have five docker container in our setup.

  The provisioning of the aws infrastructure and configuring the
  instance is done via Ansible. Ansible server runs inside the local
  workstation, which connects to aws for provisioning of server and
  its configuration.

  The architecture design diagram is shown below. We are gone a setup
  this.

  [[../arch/architecture-design-diagram.jpeg]]

** Communication network ports
   |---------------+----------------------------------------------------------------------------|
   | *Service      | Listens on*                                                                |
   |---------------+----------------------------------------------------------------------------|
   | wordpress     | 0.0.0.0:8081->80/tcp                                                       |
   |---------------+----------------------------------------------------------------------------|
   | mysqldb       | 3306/tcp                                                                   |
   |---------------+----------------------------------------------------------------------------|
   | kibana        | 0.0.0.0:5601->5601/tcp                                                     |
   |---------------+----------------------------------------------------------------------------|
   | logstash      | 0.0.0.0:5000->5000/tcp, 0.0.0.0:25826->25826/tcp, 0.0.0.0:12201->12201/udp |
   |---------------+----------------------------------------------------------------------------|
   | elasticsearch | 0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp                             |
   |---------------+----------------------------------------------------------------------------|
   | Host          | 0.0.0.0:22                                                                 |
   |---------------+----------------------------------------------------------------------------|

* COMMENT Directory structure of this repo
  #+BEGIN_EXAMPLE
.
|-- arch
|   |-- architecture-design-diagram.dia
|   |-- architecture-design-diagram.jpeg
|   `-- aws-ram-usage.png
|-- docs
|   |-- document.html
|   |-- document.org
|   |-- notes.org
|   `-- plan.org
|-- README.md
`-- src
    |-- configuration
    |   |-- configure-ec2.yml
    |   |-- deploy-app.yml
    |   |-- deploy-elk.yml
    |   `-- docker
    |       |-- application
    |       |   |-- docker-app.yml
    |       |   |-- mysql
    |       |   |   `-- Dockerfile
    |       |   `-- wordpress
    |       |       `-- Dockerfile
    |       `-- elk-stack
    |           |-- docker-elk.yml
    |           |-- elasticsearch
    |           |   `-- Dockerfile
    |           |-- kibana
    |           |   |-- config
    |           |   |   `-- kibana.yml
    |           |   |-- Dockerfile
    |           |   `-- sample-log-visualization
    |           |       |-- kibana-visualization-1.png
    |           |       `-- kibana-visualization-2.png
    |           `-- logstash
    |               |-- config
    |               |   `-- logstash.conf
    |               `-- Dockerfile
    |-- hosts
    `-- provisioning
        |-- provision.yml
        `-- vars
            |-- aws-credentials.yml
            `-- specs.yml

17 directories, 26 files
  #+END_EXAMPLE

* Infrastructure Decision
  Blogging is not a resource intensive application. It does not
  require high computation engine to run. When a user edits or publish
  a post, database is updated. Its not something which need high
  processing and memory power.

  We make some assumptions about the usage of the blogging
  application. Based on these assumptions we will decide the resources
  to be allocated to the VM. 

  Suppose there are 50 bloggers who blogs using the application. Lets
  assume they generate 10 blogs/week on an average. Suppose each blog
  would be of size 4kb. So per week 4*10 = 40kb of data will be
  generated. And per month 40*4 = 160kb. Per year it would be around
  2mb. So disk usage is not much. We can go with 512mb of hard disk.

  ELK is based on java. Java virtual machine consumes high amount of
  memory at runtime. We will need atleast 4gb of memory, with single
  processor.

* ELK Stack
** ElasticSearch
   1. JSON Document-oriented search engine
   2. Built on top of Apache Lucene
   3. Schema Free / Schema-Less
   4. Scales Up + Out
   5. refer:
      https://info.elastic.co/2016-03-AB-Test-Getting-Started-ES_Video.html?aliId=43498650

** Kibana
   1. Allows to create dashboard.
   2. We can visualize data.
   3. refer:
      https://www.elastic.co/webinars/kibana-101-get-started-with-visualizations?baymax=rtp&elektra=products&iesrc=ctr

** Logstash
   1. Log parser.
   2. Collect all logs.
   3. Make it searchable in fast and meaningful way.
   4. Use powerful analytics to summarize data across many dimensions.
   5. refer:
      https://www.elastic.co/webinars/logstash-0-60-in-60?baymax=rtp&elektra=products&iesrc=ctr
* Docker
  The concepts of docker are discussed in my another repo at:
  https://github.com/ayogi/docker/blob/master/docker.org. I am
  maintaining this since January 2016.

** Access logs
   To view the container logs:
   #+BEGIN_EXAMPLE
   docker logs -f <container-id>
   #+END_EXAMPLE

** Data volumes
   A data volume is a specially designated directory within one or
   more containers. Data volumes provide several useful features for
   persistent or shared data.

   Data volume are designed to persist data, independent of the
   container's life cycle. Docker therefore never automatically
   deletes volumes when we remove a container.

** Docker logs
   Docker captures the STDOUT and STDERR of each container process,
   stores it on disk. User can query for the logs from the host
   machine using
   #+BEGIN_EXAMPLE
   docker logs <container-id>
   #+END_EXAMPLE

   Using rsyslog service, we can forward the logs to logstash
   container. Configure =/etc/rsyslog.d/logstash.conf= file in
   wordpress container as follows:
   #+BEGIN_EXAMPLE
   *.* <ip-address-logstash>:<port>
   #+END_EXAMPLE

** Logging drivers
   The container can have a logging driver. We can use the
   =--log-driver= with the docker run command. All the logs generated
   inside the container will be sent via log driver to a remote host.
   #+BEGIN_EXAMPLE
   docker run -t -d --log-driver=syslog --log-opt syslog-address=tcp://<logstash-ip>:25826 app'
   #+END_EXAMPLE
* Workstation Requirements
  1. python2.7
  2. Ansible
     #+BEGIN_EXAMPLE
     sudo apt-get install ansible
     #+END_EXAMPLE
  3. Boto
     #+BEGIN_EXAMPLE
     sudo apt-get update
     sudo apt-get install python-boto
     #+END_EXAMPLE
  4. git
     #+BEGIN_EXAMPLE
     sudo apt-get install git
     #+END_EXAMPLE
* Setup
** EC2 specs
   - OS : ubuntu-14.04 server 64-bit
   - Hard Disk : 30 GB
   - RAM : 7.5 gb
   - Type : m3.large
   - Ports open to public:
     - ssh port 22
     - tcp ports: 5601, 9200, 8081

** Provisioning EC2
   AWS instance is launched using Ansible playbook. Ansible
   authenticates to aws using user access and secret key. 

   1. Create =aws-credentials.yml= file inside vars folder as follows:
      #+BEGIN_EXAMPLE
      AWS_ACCESS_KEY_ID=<key-id>
      AWS_SECRET_ACCESS_KEY=<secret-key>
      #+END_EXAMPLE
      Or, we can set the environment variable
      #+BEGIN_EXAMPLE
      export AWS_ACCESS_KEY_ID=<key-id>
      export AWS_SECRET_ACCESS_KEY=<secret-key>
      #+END_EXAMPLE

   2. Run playbook
      #+BEGIN_EXAMPLE
      ansible-playbook -i hosts provision.yml -vvvv
      #+END_EXAMPLE

   3. Now make a host entry in the hosts file, for the newly created
      instance as given below. Replace the ip with the public-ip of
      newly created instance.
      #+BEGIN_EXAMPLE
      [vm]
      52.39.75.171
      #+END_EXAMPLE

** Configuring EC2
   EC2 instance is configured via Ansible playbook. It installs the
   docker-engine, docker-compose and other required packages.

   Run playbook to configure the ec2 container and setup docker.
   #+BEGIN_EXAMPLE
   ansible-playbook configure-ec2.yml -i hosts --private-key  <path-to-keypair>
   #+END_EXAMPLE

   Manual process to do the configuration is describe below:

*** Install docker
    1. Install docker 
       #+BEGIN_EXAMPLE
       $ sudo apt-get update
       $ sudo apt-get install apt-transport-https ca-certificates
       $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
       #+END_EXAMPLE

    2. Edit =/etc/apt/sources.list.d/docker.list= file and add
       following line in it
       #+BEGIN_EXAMPLE
       deb https://apt.dockerproject.org/repo ubuntu-trusty main
       #+END_EXAMPLE

    3. Update and start docker service.
       #+BEGIN_EXAMPLE
       $ sudo apt-get update
       $ sudo apt-get purge lxc-docker
       $ apt-cache policy docker-engine
       $ sudo apt-get install linux-image-extra-$(uname -r)
       $ sudo reboot
       $ sudo apt-get update       
       $ sudo apt-get install docker-engine
       $ sudo service docker start
       $ sudo docker run hello-world
       #+END_EXAMPLE

    4. Configure group, and then logout and logback in
       #+BEGIN_EXAMPLE
       $ sudo usermod -aG docker ubuntu
       #+END_EXAMPLE

*** Install docker compose
    #+BEGIN_EXAMPLE
    $ sudo su -
    $ curl -L https://github.com/docker/compose/releases/download/1.7.1/docker-compose-Linux-x86_64 > /usr/local/bin/docker-compose
    $ chmod +x /usr/local/bin/docker-compose
    $ docker-compose --version
    #+END_EXAMPLE
** Create containers
*** ELK Stack
    ELK stack is configured via Ansible playbook. Which internally
    uses docker-compose file to setup elk stack. Customized images are
    build from the official docker images. Then services inside the
    container are started.
    
    The services running inside the containers needs to communicate
    between each other. Logstash sends its parsed logs to
    elasticsearch container. That means they need to communicate over
    some specified port. Docker provides a way to allow communication
    between containers, by setting =links= option. Logs from the
    application container are forwarded to logstash container using
    syslog log driver. When we links client container to the server,
    will see an entry in =/etc/hosts= file of the client.

    To do the setup run ansible playbook as follows:
    #+BEGIN_EXAMPLE
    ansible-playbook -i ../hosts deploy-elk.yml --private-key <keypair>
    #+END_EXAMPLE

    Manual process to do the setup using compose file is described
    below.
    
    - Command to build images
      #+BEGIN_EXAMPLE
      docker-compose -f docker-elk.yml build
      #+END_EXAMPLE
    - Command to start services in background.
      #+BEGIN_EXAMPLE
      docker-compose -f docker-elk.yml up -d
      #+END_EXAMPLE
    - Adding logs to logstash.
      #+BEGIN_EXAMPLE
      nc localhost 5000 < /var/log/auth.log
      #+END_EXAMPLE

**** Elasticsearch configuration
     We are not configuring elasticsearch, we are using the default
     configuration.

**** Logstash configuration
     Logstash configuration file is at
     =/etc/logstash/conf.d/logstash.conf=. We have configured its
     input, filter and output parts. Input we setup to accept incoming
     logs on tcp port 5000 and syslog logs on
     port 25826. Corresponding to each incoming port we have to enable
     the port mapping in the dockerfile. Logstash process the logs and
     then sends it to elasticsearch on port 5601.

**** Kibana configuration
     Kibana configuration file is at
     =/opt/kibana/config/kibana.yml=. We have configured following
     directives inside it.
     1. Port the kibana will run on:
        #+BEGIN_EXAMPLE
	server.port: 5601
	#+END_EXAMPLE

     2. The host to bind to and will accept connection from
	#+BEGIN_EXAMPLE
	server.host: "0.0.0.0"
	#+END_EXAMPLE

     3. Elasticsearch url
	#+BEGIN_EXAMPLE
	elasticsearch.url: 'http://elasticsearch:9200'
	#+END_EXAMPLE

     4. Preserving elasticsearch host - I really don't get this, what
        it does.
	#+BEGIN_EXAMPLE
	elasticsearch.preserveHost: true
	#+END_EXAMPLE

     5. Kibana index name and default application
	#+BEGIN_EXAMPLE
	kibana.index: ".kibana"
	kibana.defaultAppId: "discover"
	#+END_EXAMPLE
      
*** Application
    Application is configured via ansible playbook. Which internally
    uses docker-compose file. It setups the wordress application in
    one container and database service inside another container.

    Run the ansible playbook as follows:
    #+BEGIN_EXAMPLE
    ansible-playbook -i ../hosts deploy-app.yml --private-key <keypair>
    #+END_EXAMPLE

    Manual process to do the setup using compose file is described
    below.

    - Command to build images
      #+BEGIN_EXAMPLE
      docker-compose -f docker-app.yml build
      #+END_EXAMPLE

    - Command to start services in background.
      #+BEGIN_EXAMPLE
      docker-compose -f docker-app.yml up -d
      #+END_EXAMPLE

**** Wordpress configuration
     We are using default configuration of the wordpress. We are not
     configuring it. For wordpress to work, host machine's port 8081
     is mapped to port 80 of docker container. Wordpress container is
     linked to db container.
     
**** Mysql configuration
     Mysql root password is set.

* Install Wordpress
  After the setup we can go to url: [[http://<vm-ip>:8081]] to install
  the wordpress.

* Visualize logs in Kibana
  After setting everything we can now open the url -
  [[http://<vm-ip>:5601]] to visualize the logs in kibana.

  We can also open [[http://<vm-ip>:9200/_search?q%3D*&pretty][http://<vm-ip>:9200/_search?q=*&pretty]] to see the
  log entries in elasticsearch.

* Filebeat log collector
  I tried configuring logstash with filebeat input but when I tested
  the setup, there were no logs getting saved in elasticsearch.

** Logstash container
   Following steps were performed to configure filebeat on logstash
   container:
   1. Create certificates directory:
      #+BEGIN_EXAMPLE
      $ sudo mkdir -p /etc/pki/tls/certs
      $ sudo mkdir /etc/pki/tls/private
      #+END_EXAMPLE

   2. Edit =/etc/ssl/openssl.cnf= and find the =v3_ca= and add
      following line under it
      #+BEGIN_EXAMPLE
      subjectAltName = IP: ELK_server_private_IP
      #+END_EXAMPLE

   3. Now generate ssl certificate and private key using following
      command
      #+BEGIN_EXAMPLE
      cd /etc/pki/tls
      sudo openssl req -config /etc/ssl/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
      #+END_EXAMPLE

   4. Configure =/etc/logstash/conf.d/logstash.conf= as follows:
      #+BEGIN_EXAMPLE
      input {
      tcp {
      port => 5000
      }
      gelf {}
       syslog {
       port => 25826
       type => syslog
       }
       beats {
       port => 5044
       ssl => true
       ssl_certificate => "/etc/pki/tls/certs/logstash-forwarder.crt"
       ssl_key => "/etc/pki/tls/private/logstash-forwarder.key"
       }
       
       }
     
       filter {
       if [type] == "syslog" {
       grok {
       match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
       add_field => [ "received_at", "%{@timestamp}" ]
       add_field => [ "received_from", "%{host}" ]
       }
       syslog_pri { }
       date {
       match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
       }
       }
       }
       
       
       output {
       elasticsearch {
       hosts => "elasticsearch:9200"
       }
       }
       #+END_EXAMPLE

   5. Test configuration:
      #+BEGIN_EXAMPLE
      $ service logstash configtest                                                                                                
      Configuration OK
      #+END_EXAMPLE
  
   6. Restart logstash service
      #+BEGIN_EXAMPLE
      sudo service logstash restart
      #+END_EXAMPLE
  
** Wordpress container
   1. Copy ssl certificates from logstash container to the wordpress
      container:
      #+BEGIN_EXAMPLE
      $ sudo mkdir -p /etc/pki/tls/certs
      #+END_EXAMPLE
      Then copy =/etc/pki/tls/certs/logstash-forwarder.crt= here from
      logstash container.

   2. Install filebeat package
      #+BEGIN_EXAMPLE
      echo "deb https://packages.elastic.co/beats/apt stable main" |  sudo tee -a /etc/apt/sources.list.d/beats.list
      wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
      sudo apt-get update
      sudo apt-get install filebeat
      #+END_EXAMPLE

   3. Configure =/etc/filebeat/filebeat.yml= as follows:
      #+BEGIN_EXAMPLE
      filebeat:
      prospectors:
      -
      paths:
      - /var/log/auth.log
      - /var/log/syslog
      
      input_type: log
      
      document_type: syslog
   
      registry_file: /var/lib/filebeat/registry
      output:
      logstash:
      hosts: ["172.17.0.4:5044"]  # logstash machine ip
      bulk_max_size: 1024
      
      tls:
      certificate_authorities: ["/etc/pki/tls/certs/logstash-forwarder.crt"]
      
      shipper:
      
      logging:
      files:
      rotateeverybytes: 10485760 # = 10MB
      #+END_EXAMPLE
  
   4. Restart filebeat service:
      #+BEGIN_EXAMPLE
      $ sudo service filebeat restart
      Restarting Sends log files to Logstash or directly to Elasticsearch.: filebeat.
      #+END_EXAMPLE
  
   5. Testing filebeat installation shows nothing
      #+BEGIN_EXAMPLE
      curl -XGET 'http://localhost:9200/filebeat-*/_search?pretty'
      {
      "took" : 1,
      "timed_out" : false,
      "_shards" : {
      "total" : 0,
      "successful" : 0,
      "failed" : 0
      },
      "hits" : {
      "total" : 0,
      "max_score" : 0.0,
      "hits" : [ ]
      }
      }
      #+END_EXAMPLE

* References
  1. https://hub.docker.com/_/wordpress/ (wordpress official image)
  2. https://hub.docker.com/_/elasticsearch/ (elasticsearch official
     image)
  3. https://hub.docker.com/_/logstash/ (logstash official image)
  4. https://hub.docker.com/_/kibana/ (kibana official image)
  5. https://docs.docker.com/engine/admin/logging/overview/ (logging
     in docker)
  6. https://docs.docker.com/engine/userguide/containers/dockervolumes/
     (docker volumes)
  7. https://info.elastic.co/2016-03-AB-Test-Getting-Started-ES_Video.html?aliId=43498650
     (elasticsearch introduction)
  8. https://www.elastic.co/webinars/introduction-elk-stack
     (elasticsearch introduction)
  9. https://www.linode.com/docs/databases/elasticsearch/visualizing-apache-webserver-logs-in-the-elk-stack-on-debian-8
     (visualize logs in kibana)
  10. http://docs.ansible.com/ansible/lineinfile_module.html (ansible
      lineinfile module)
  11. http://docs.ansible.com/ansible/apt_module.html (ansible apt
      module)
  12. http://docs.ansible.com/ansible/guide_aws.html (ansible aws
      guide)
  13. http://docs.ansible.com/ansible/authorized_key_module.html
      (ansible authorized key module)
  14. http://docs.ansible.com/ansible/user_module.html (ansible user
      module)
