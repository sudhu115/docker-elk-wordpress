#+Title: Docker ELK Wordpress Project
#+Author: Yogesh Agrawal
#+Email: yogeshiiith@gmail.com
#+Date: <2016-06-16 Thu>

* Introduction
  This document describes the design and implementation of the
  docker compose based application deployment and centralized logging
  using Elasticsearch, Logstash and Kibana stack.

* Directory structure of this repo
  #+BEGIN_EXAMPLE
.
|-- arch
|   |-- architecture-design-diagram.dia
|   |-- architecture-design-diagram.jpeg
|   `-- aws-ram-usage.png
|-- docs
|   |-- document.html
|   |-- document.org
|   |-- notes.org
|   `-- plan.org
|-- README.md
`-- src
    |-- configuration
    |   |-- configure-ec2.yml
    |   |-- deploy-app.yml
    |   |-- deploy-elk.yml
    |   `-- docker
    |       |-- application
    |       |   |-- docker-app.yml
    |       |   |-- mysql
    |       |   |   `-- Dockerfile
    |       |   `-- wordpress
    |       |       `-- Dockerfile
    |       `-- elk-stack
    |           |-- docker-elk.yml
    |           |-- elasticsearch
    |           |   `-- Dockerfile
    |           |-- kibana
    |           |   |-- config
    |           |   |   `-- kibana.yml
    |           |   |-- Dockerfile
    |           |   `-- sample-log-visualization
    |           |       |-- kibana-visualization-1.png
    |           |       `-- kibana-visualization-2.png
    |           `-- logstash
    |               |-- config
    |               |   `-- logstash.conf
    |               `-- Dockerfile
    |-- hosts
    `-- provisioning
        |-- provision.yml
        `-- vars
            |-- aws-credentials.yml
            `-- specs.yml

17 directories, 26 files
  #+END_EXAMPLE
* Design
  The infrastructure is setup in the cloud environment - AWS. An EC2
  instance is created with the minimal requirement. The rational
  behind resource requirement of an EC2 instance is discussed below.
  
  Inside the EC2 instance, the services are setup in a container based
  system. Here we have mainly five services:
  1. Elasticsearch
  2. Logstash
  3. Kibana
  4. Application - Wordpress
  5. Mysql database

  So we will have four docker container in our setup.

  The provisioning of the aws infrastructure and configuring the
  instance is done via Ansible. Ansible server runs inside the local
  workstation, which connects to aws for provision of server and
  configuring it.

  The architecture design diagram is shown below:

  [[../arch/architecture-design-diagram.jpeg]]

** Communication network ports
   |---------------+----------------------------------------------------------------------------|
   | *Service      | Listens on*                                                             |
   |---------------+----------------------------------------------------------------------------|
   | wordpress     | 0.0.0.0:8081->80/tcp                                                       |
   |---------------+----------------------------------------------------------------------------|
   | mysqldb       | 3306/tcp                                                                   |
   |---------------+----------------------------------------------------------------------------|
   | kibana        | 0.0.0.0:5601->5601/tcp                                                     |
   |---------------+----------------------------------------------------------------------------|
   | logstash      | 0.0.0.0:5000->5000/tcp, 0.0.0.0:25826->25826/tcp, 0.0.0.0:12201->12201/udp |
   |---------------+----------------------------------------------------------------------------|
   | elasticsearch | 0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp                             |
   |---------------+----------------------------------------------------------------------------|
   | Host          | 0.0.0.0:22                                                                 |
   |---------------+----------------------------------------------------------------------------|


* Infrastructure Decision
  Blogging is not a resource intensive application. It does not
  require high computation engine to run. When a user edits or publish
  a post, database is updated. Its not something which need high
  processing, memory power.

  We make some assumptions about the usage of the blogging
  application. Based on these assumptions we will decide the resources
  to be allocated to the VM. 

  Suppose there are 50 bloggers who blogs using the application. Lets
  assume they generate 10 blogs/week on an average. Each blog would be
  of size 4kb. So per week 4*10 = 40kb of data will be generated. And
  per month 40*4 = 160kb. Per year it would be around 2mb. So disk
  usage is not much. We can go with 512mb of hard disk.

  ELK is based on java. Java virtual machine consumes high amount of
  memory at runtime. We will need atleast 4gb of memory, with single
  processor.

* ELK Stack
** ElasticSearch
   1. JSON Document-oriented search engine
   2. Built on top of Apache Lucene
   3. Schema Free / Schema-Less
   4. Scales Up + Out
   5. refer:
      https://info.elastic.co/2016-03-AB-Test-Getting-Started-ES_Video.html?aliId=43498650

** Kibana
   1. Allows to create dashboard.
   2. We can visualize data.
   3. refer:
      https://www.elastic.co/webinars/kibana-101-get-started-with-visualizations?baymax=rtp&elektra=products&iesrc=ctr

** Logstash
   1. Log parser.
   2. Collect all logs.
   3. Make it searchable in fast and meaningful way.
   4. Use powerful analytics to summarize data across many dimensions.
   5. refer:
      https://www.elastic.co/webinars/logstash-0-60-in-60?baymax=rtp&elektra=products&iesrc=ctr
* Docker
  The concepts of docker are discussed in my another repo at:
  https://github.com/ayogi/docker/blob/master/docker.org. I am
  maintaining this from January 2016.

** Access logs
   To view the container logs:
   #+BEGIN_EXAMPLE
   docker logs -f <container-id>
   #+END_EXAMPLE

** Data volumes
   A data volume is a specially designated directory within one or
   more containers. Data volumes provide several useful features for
   persistent or shared data.

   Data volume are designed to persist data, independent of the
   container's life cycle. Docker therefore never automatically
   deletes volumes when you remove a container.

** Docker logs
   Docker captures the STDOUT and STDERR of each container process,
   stores it on disk. User can query for the logs from the host
   machine using
   #+BEGIN_EXAMPLE
   docker logs <container-id>
   #+END_EXAMPLE

   Using rsyslog service, we can forward the logs to logstash
   container. Configure =/etc/rsyslog.d/logstash.conf= file in
   wordpress container as follows:
   #+BEGIN_EXAMPLE
   *.* <ip-address-logstash>:<port>
   #+END_EXAMPLE

** Logging drivers
   The container can have a logging driver. We can use the
   =--log-driver= with the docker run command. All the logs generated
   inside the container will be sent via log driver to a remote host.
   #+BEGIN_EXAMPLE
   docker run -t -d --log-driver=syslog --log-opt syslog-address=tcp://172.17.0.4:25826 app'
   #+END_EXAMPLE
* Workstation Requirements
  1. python2.7
  2. Ansible server
     #+BEGIN_EXAMPLE
     sudo apt-get install ansible
     #+END_EXAMPLE
  3. Boto
     #+BEGIN_EXAMPLE
     sudo apt-get update
     sudo apt-get install python-boto
     #+END_EXAMPLE
  4. git
     #+BEGIN_EXAMPLE
     sudo apt-get install git
     #+END_EXAMPLE
* Setup
** EC2 specs
   - OS : ubuntu-14.04 server 64-bit
   - Hard Disk : 30 GB
   - RAM : 7.5 gb
   - Type : m3.large
   - Ports open to outside world:
     - ssh port 22
     - tcp ports: 5601, 9200, 8081

** Provisioning EC2
   AWS instance is launched using Ansible playbook. Ansible
   authenticates to aws using user access and secret key. 

   1. Create =aws-credentials.yml= file inside vars folder as follows:
      #+BEGIN_EXAMPLE
      AWS_ACCESS_KEY_ID=<key-id>
      AWS_SECRET_ACCESS_KEY=<secret-key>
      #+END_EXAMPLE
      Or, we can set the environment variable
      #+BEGIN_EXAMPLE
      export AWS_ACCESS_KEY_ID=<key-id>
      export AWS_SECRET_ACCESS_KEY=<secret-key>
      #+END_EXAMPLE

   2. Run playbook
      #+BEGIN_EXAMPLE
      ansible-playbook -i hosts provision.yml -vvvv
      #+END_EXAMPLE

   3. Now make a host entry in the hosts file, for the newly created
      instance as given below. Replace the ip with the public-ip of
      newly created instance.
      #+BEGIN_EXAMPLE
      [vm]
      52.39.75.171
      #+END_EXAMPLE

** Configuring EC2
   EC2 instance is configured via Ansible playbook. It installs the
   docker-engine, docker-compose and other required packages.

   Run playbook to configure the ec2 container and setup docker.
   #+BEGIN_EXAMPLE
   ansible-playbook configure-ec2.yml -i hosts --private-key  <path-to-keypair>
   #+END_EXAMPLE

   Manual process to do the configuration is describe below:

*** Install docker
    1. Install docker 
       #+BEGIN_EXAMPLE
       $ sudo apt-get update
       $ sudo apt-get install apt-transport-https ca-certificates
       $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
       #+END_EXAMPLE

    2. Edit =/etc/apt/sources.list.d/docker.list= file
       #+BEGIN_EXAMPLE
       deb https://apt.dockerproject.org/repo ubuntu-trusty main
       #+END_EXAMPLE

    3. Update and start service.
       #+BEGIN_EXAMPLE
       $ sudo apt-get update
       $ sudo apt-get purge lxc-docker
       $ apt-cache policy docker-engine
       $ sudo apt-get install linux-image-extra-$(uname -r)
       $ sudo reboot
       $ sudo apt-get update       
       $ sudo apt-get install docker-engine
       $ sudo service docker start
       $ sudo docker run hello-world
       #+END_EXAMPLE

    4. Configure group, and then logout and logback in
       #+BEGIN_EXAMPLE
       $ sudo usermod -aG docker ubuntu
       #+END_EXAMPLE

*** Install docker compose
    #+BEGIN_EXAMPLE
    $ sudo su -
    $ curl -L https://github.com/docker/compose/releases/download/1.7.1/docker-compose-Linux-x86_64 > /usr/local/bin/docker-compose
    $ chmod +x /usr/local/bin/docker-compose
    $ docker-compose --version
    #+END_EXAMPLE
** Create containers
*** ELK Stack
    ELK stack is configured via Ansible playbook. Which internally
    uses docker-compose file to setup elk stack. Customized images are
    build from the official docker images. Then services inside the
    container are started.
    
    The services running inside the containers needs to communicate
    between each other. Logstash sends its parsed logs to
    elasticsearch container. That means they need to communicate over
    some specified port. Docker provides a way to allow communication
    between containers, by setting =links= option. Logs from the
    application container are forwarded to logstash container using
    syslog log driver. When we links client container to the server,
    will see an entry in =/etc/hosts= file of the client.

    To do the setup run ansible playbook as follows:
    #+BEGIN_EXAMPLE
    ansible-playbook -i ../hosts deploy-elk.yml --private-key <keypair>
    #+END_EXAMPLE

    Manual process to do the setup using compose file is described
    below.
    
    - Command to build images
      #+BEGIN_EXAMPLE
      docker-compose -f docker-elk.yml build
      #+END_EXAMPLE
    - Command to start services in background.
      #+BEGIN_EXAMPLE
      docker-compose -f docker-elk.yml up -d
      #+END_EXAMPLE
    - Adding logs to logstash.
      #+BEGIN_EXAMPLE
      nc localhost 5000 < /var/log/auth.log
      #+END_EXAMPLE

**** Elasticsearch configuration
     We are not configuring elasticsearch, we are using the default
     configuration.

**** Logstash configuration
     Logstash configuration file is at
     =/etc/logstash/conf.d/logstash.conf=. We have configured its
     input, filter and output parts. Input we setup to accept incoming
     logs on tcp port 5000 and syslog logs on
     port 25826. Corresponding to each incoming port we have to enable
     the port mapping in the dockerfile. Logstash process the logs and
     then sends it to elastic search on port 5601.

**** Kibana configuration
     Kibana configuration file is at
     =/opt/kibana/config/kibana.yml=. We have configured following
     directives inside it.
     1. Port the kibana will run on:
        #+BEGIN_EXAMPLE
	server.port: 5601
	#+END_EXAMPLE
     2. The host to bind to and will accept connection from
	#+BEGIN_EXAMPLE
	server.host: "0.0.0.0"
	#+END_EXAMPLE
     3. Elasticsearch url
	#+BEGIN_EXAMPLE
	elasticsearch.url: 'http://elasticsearch:9200'
	#+END_EXAMPLE
     4. Preserving elasticsearch host - I really don't get this, what
        it does.
	#+BEGIN_EXAMPLE
	elasticsearch.preserveHost: true
	#+END_EXAMPLE
     5. Kibana index name and default application
	#+BEGIN_EXAMPLE
	kibana.index: ".kibana"
	kibana.defaultAppId: "discover"
	#+END_EXAMPLE
      
*** Application
    Application is configured via ansible playbook. Which internally
    uses docker-compose file. It setups the wordress application in
    one container and database service inside another container.

    Run the ansible playbook as follows:
    #+BEGIN_EXAMPLE
    ansible-playbook -i ../hosts deploy-app.yml --private-key <keypair>
    #+END_EXAMPLE

    Manual process to do the setup using compose file is described
    below.

    - Command to build images
      #+BEGIN_EXAMPLE
      docker-compose -f docker-app.yml build
      #+END_EXAMPLE

    - Command to start services in background.
      #+BEGIN_EXAMPLE
      docker-compose -f docker-app.yml up -d
      #+END_EXAMPLE

* Install Wordpress
  After the setup we can go to url: [[http://<vm-ip>:8081]] to install
  the wordpress.

* Visualize logs in Kibana
  After setting everything we can now open the url -
  [[http://<vm-ip>:5601]] to visualize the logs in kibana.

  We can also open [[http://<vm-ip>:9200/_search?q%3D*&pretty][http://<vm-ip>:9200/_search?q=*&pretty]] to see the
  log entries in elasticsearch.

* References
  1. https://hub.docker.com/_/wordpress/
  2. https://hub.docker.com/_/elasticsearch/
  3. https://hub.docker.com/_/logstash/
  4. https://hub.docker.com/_/kibana/
  5. https://info.elastic.co/2016-03-AB-Test-Getting-Started-ES_Video.html?aliId=43498650
  6. https://docs.docker.com/engine/userguide/containers/dockervolumes/
  7. https://www.elastic.co/webinars/introduction-elk-stack
  8. https://www.linode.com/docs/databases/elasticsearch/visualizing-apache-webserver-logs-in-the-elk-stack-on-debian-8
  9. https://docs.docker.com/engine/userguide/containers/dockervolumes/
  10. https://docs.docker.com/engine/admin/logging/overview/
  11. http://docs.ansible.com/ansible/lineinfile_module.html
  12. http://docs.ansible.com/ansible/apt_module.html
  13. http://docs.ansible.com/ansible/guide_aws.html
  14. http://docs.ansible.com/ansible/authorized_key_module.html
  15. http://docs.ansible.com/ansible/user_module.html
