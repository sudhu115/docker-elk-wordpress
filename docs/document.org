#+Title: Wordpress Cloud Deployment Project
#+Author: Yogesh Agrawal
#+Email: yogeshiiith@gmail.com
#+Date: <2016-06-16 Thu>

* Introduction
  This document describes the design and implementation of the
  docker compose based application deployment and centralized logging
  using Elasticsearch, Logstash and Kibana stack.

* Design
  The infrastructure is setup in the cloud environment - AWS. An EC2
  instance is created with the minimal requirement. The rational
  behind resource requirement of an EC2 instance is discussed below.
  
  Inside the EC2 instance, the services are setup in a container based
  system. Here we have mainly five services:
  1. Elasticsearch
  2. Logstash
  3. Kibana
  4. Application - Wordpress
  5. Mysql database

  So we will have four docker container in our setup.

  The provisioning of the aws infrastructure and configuring the
  instance is done via Ansible. Ansible server runs inside the local
  workstation, which connects to aws for provision of server and
  configuring it.

  The architecture design diagram is shown below:

  [[../arch/architecture-design-diagram.jpeg]]

* Infrastructure Decision
  Blogging is not a resource intensive application. It does not
  require high computation engine to run. When a user edits or publish
  a post, database is updated. Its not something which need high
  processing, memory power.

  We make some assumptions about the usage of the blogging
  application. Based on these assumptions we will decide the resources
  to be allocated to the VM. 

  Suppose there are 50 bloggers who blogs using the application. Lets
  assume they generate 10 blogs/week on an average. Each blog would be
  of size 4kb. So per week 4*10 = 40kb of data will be generated. And
  per month 40*4 = 160kb. Per year it would be around 2mb. So disk
  usage is not much. We can go with 512mb of hard disk.

  ELK is based on java. Java virtual machine consumes high amount of
  memory at runtime. We will need atleast 4gb of memory, with single
  processor.

* ELK Stack
** ElasticSearch
   1. JSON Document-oriented search engine
   2. Built on top of Apache Lucene
   3. Schema Free / Schema-Less
   4. Scales Up + Out
   5. refer:
      https://info.elastic.co/2016-03-AB-Test-Getting-Started-ES_Video.html?aliId=43498650

** Kibana
   1. Allows to create dashboard.
   2. We can visualize data.
   3. refer:
      https://www.elastic.co/webinars/kibana-101-get-started-with-visualizations?baymax=rtp&elektra=products&iesrc=ctr

** Logstash
   1. Log parser.
   2. Collect all logs.
   3. Make it searchable in fast and meaningful way.
   4. Use powerful analytics to summarize data across many dimensions.
   5. refer:
      https://www.elastic.co/webinars/logstash-0-60-in-60?baymax=rtp&elektra=products&iesrc=ctr
* Docker
  The concepts of docker are discussed in my another repo at:
  https://github.com/ayogi/docker/blob/master/docker.org. I am
  maintaining this from January 2016.

** Access logs
   To view the container logs:
   #+BEGIN_EXAMPLE
   docker logs -f <container-id>
   #+END_EXAMPLE

** Data volumes
   A data volume is a specially designated directory within one or
   more containers. Data volumes provide several useful features for
   persistent or shared data.

   Data volume are designed to persist data, independent of the
   container's life cycle. Docker therefore never automatically
   deletes volumes when you remove a container.

** Docker logs
   Docker captures the STDOUT and STDERR of each container process,
   stores it on disk. User can query for the logs from the host
   machine using
   #+BEGIN_EXAMPLE
   docker logs <container-id>
   #+END_EXAMPLE

   Using rsyslog service, we can forward the logs to logstash
   container. Configure =/etc/rsyslog.d/logstash.conf= file in
   wordpress container as follows:
   #+BEGIN_EXAMPLE
   *.* <ip-address-logstash>:<port>
   #+END_EXAMPLE

** Logging drivers
   The container can have a logging driver. We can use the
   =--log-driver= with the docker run command. All the logs generated
   inside the container will be sent via log driver to a remote host.
   #+BEGIN_EXAMPLE
   docker run -t -d --log-driver=syslog --log-opt syslog-address=tcp://172.17.0.4:25826 app'
   #+END_EXAMPLE
* Setup
** EC2 specs
   - OS : ubuntu-14.04 server 64-bit
   - Hard Disk : 30 GB
   - RAM : 4gb
   - Type : t1.micro
   - Open ports :
      - 5000: Logstash.
      - 9200: Elasticsearch HTTP
      - 9300: Elasticsearch TCP transport
      - 5601: Kibana
      - 80: http
      - 22: ssh

** Launch instance
   AWS instance is launched using Ansible playbook. Ansible
   authenticates to aws using user access and secret key.

** Configure instance
   Instance is configured via Ansible playbook. We install docker
   engine inside the vm to create docker containers.

*** Install docker 1.10
    1. Install docker 
       #+BEGIN_EXAMPLE
       $ sudo apt-get update
       $ sudo apt-get install apt-transport-https ca-certificates
       $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
       #+END_EXAMPLE

    2. Edit =/etc/apt/sources.list.d/docker.list= file
       #+BEGIN_EXAMPLE
       deb https://apt.dockerproject.org/repo ubuntu-trusty main
       #+END_EXAMPLE

    3. Update and start service.
       #+BEGIN_EXAMPLE
       $ sudo apt-get update
       $ sudo apt-get purge lxc-docker
       $ apt-cache policy docker-engine
       $ sudo apt-get install linux-image-extra-$(uname -r)
       $ sudo apt-get update
       $ sudo apt-get install docker-engine
       $ sudo service docker start
       $ sudo docker run hello-world
       #+END_EXAMPLE

    4. Configure group, and then logout and logback in
       #+BEGIN_EXAMPLE
       $ sudo usermod -aG docker ubuntu
       #+END_EXAMPLE

*** Install docker compose
    #+BEGIN_EXAMPLE
    $ sudo su -
    $ curl -L https://github.com/docker/compose/releases/download/1.7.1/docker-compose-Linux-x86_64 > /usr/local/bin/docker-compose
    $ chmod +x /usr/local/bin/docker-compose
    $ docker-compose --version
    #+END_EXAMPLE
** Create containers
*** ELK Stack
    ELK stack is configured via docker-compose file. Compose file
    setup all the stack components one by one. Compose file builds the
    customized images using the official images. It then creates the
    container and configures them for our use.
    
    The services running inside the containers needs to communicate
    between each other. Logstash sends its parsed logs to
    elasticsearch container. That means they need to communicate over
    some specified port. Docker provides a way to allow communication
    between containers, by setting =links= option. When we links
    client container to the server, will see an entry in =/etc/hosts=
    file of the client, something similar to.

    - Command to build images
      #+BEGIN_EXAMPLE
      docker-compose -f docker-elk.yml build
      #+END_EXAMPLE
    - Command to start services in background.
      #+BEGIN_EXAMPLE
      docker-compose -f docker-elk.yml up -d
      #+END_EXAMPLE
    - Adding logs to logstash.
      #+BEGIN_EXAMPLE
      nc localhost 5000 < /var/log/auth.log
      #+END_EXAMPLE
      
*** Application
    Application is configured via docker-compose file. It setups the
    required wordress application in one container and database in
    another container.

    Replace the "ip-address" in the docker-app.yml file with the ip
    address of logstash machine. Use following command to get the ip
    address of logstash machine.

    #+BEGIN_EXAMPLE
    docker inspect logstash | grep IPAddress
    #+END_EXAMPLE

    - Command to build images
      #+BEGIN_EXAMPLE
      docker-compose -f docker-app.yml build
      #+END_EXAMPLE

    - Command to start services in background.
      #+BEGIN_EXAMPLE
      docker-compose -f docker-app.yml up -d
      #+END_EXAMPLE

* Install Wordpress
  After the setup we can go to url: http::/localhost:8081 to install
  the wordpress.

* Visualizing logs in kibana
  After setting everything we can now open the url -
  http:://localhost:5601 to visualize the logs in kibana.

  We can also open http://localhost:9200/_search?q=*&pretty to see the
  log entries in elasticsearch.

* References
  1. https://info.elastic.co/2016-03-AB-Test-Getting-Started-ES_Video.html?aliId=43498650
  2. https://docs.docker.com/engine/userguide/containers/dockervolumes/
  3. https://www.elastic.co/webinars/introduction-elk-stack
  4. https://www.linode.com/docs/databases/elasticsearch/visualizing-apache-webserver-logs-in-the-elk-stack-on-debian-8
  5. https://docs.docker.com/engine/userguide/containers/dockervolumes/
  6. https://docs.docker.com/engine/admin/logging/overview/
